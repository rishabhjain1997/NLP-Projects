{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Translation using Sequence to Sequence Learning in Keras with Encoders and Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blog link - https://www.youtube.com/redirect?q=https%3A%2F%2Fblog.keras.io%2Fa-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html&v=f-JCCOHwx1c&redir_token=QUFFLUhqbDYydnd2WkZHSHhZZEd6ZklJNzBfMXJHc2g5UXxBQ3Jtc0tuVzFNSlQxVkgxVFpkdXA2OG5VQVZmNUFWcmpGQUV3TmdkQ1AxZ21RT3J6YTZ2Ung4djBSZnhDQXlQci05UF8taGNuRFF6WnVsMFlROXEyV2xDZ3NIRndTaTB5QlFtRElwWW4wMXE3UEsxRlhuRnp2Zw%3D%3D&event=video_description\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "latent_dim = 256\n",
    "num_samples = 10000\n",
    "data_path = 'fra-eng/fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "with open(data_path, 'r', encoding = 'utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[:min(num_samples, len(lines) -1)]:\n",
    "    input_text, target_text, *_ = line.split('\\t')\n",
    "    target_text = '\\t'+ target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(set(\"\".join(input_texts))))\n",
    "target_characters = sorted(list(set(\"\".join(target_texts))))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max(list(map(len, input_texts)))\n",
    "max_decoder_seq_length = max(list(map(len, target_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  10000\n",
      "Number of unique input tokens:  71\n",
      "Number of unique target tokens:  94\n",
      "Max sequence length for inputs:  15\n",
      "Max sequence length for target:  59\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples: ', len(input_texts))\n",
    "print('Number of unique input tokens: ', num_encoder_tokens)\n",
    "print('Number of unique target tokens: ', num_decoder_tokens)\n",
    "print('Max sequence length for inputs: ', max_encoder_seq_length)\n",
    "print('Max sequence length for target: ', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = (dict(enumerate(input_characters)))\n",
    "input_token_index = dict(zip(input_token_index.values(), input_token_index.keys()))\n",
    "target_token_index = (dict(enumerate(target_characters)))\n",
    "target_token_index = dict(zip(target_token_index.values(), target_token_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype = 'float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype = 'float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1\n",
    "    encoder_input_data[i, t+1:, input_token_index[' ']] = 1\n",
    "    \n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1\n",
    "    decoder_input_data[i, t+1: , target_token_index[' ']] = 1\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an Encoder Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it\n",
    "encoder_inputs = Input(shape = (None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state = True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard 'encoder_outputs' and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using 'encoder_states' as initial state\n",
    "decoder_inputs = Input(shape = (None, num_decoder_tokens))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we eill use them in inference\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences = True, return_state = True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation = 'softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 39s 312ms/step - loss: 1.1511 - accuracy: 0.7278 - val_loss: 1.0288 - val_accuracy: 0.7234\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 34s 276ms/step - loss: 0.8365 - accuracy: 0.7741 - val_loss: 0.8064 - val_accuracy: 0.7743\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.6713 - accuracy: 0.8107 - val_loss: 0.7184 - val_accuracy: 0.7939\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 40s 322ms/step - loss: 0.5905 - accuracy: 0.8293 - val_loss: 0.6610 - val_accuracy: 0.8058\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 53s 420ms/step - loss: 0.5406 - accuracy: 0.8424 - val_loss: 0.6118 - val_accuracy: 0.8185\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 55s 439ms/step - loss: 0.5042 - accuracy: 0.8521 - val_loss: 0.5934 - val_accuracy: 0.8241\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 52s 414ms/step - loss: 0.4756 - accuracy: 0.8599 - val_loss: 0.5637 - val_accuracy: 0.8338\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 54s 428ms/step - loss: 0.4506 - accuracy: 0.8668 - val_loss: 0.5422 - val_accuracy: 0.8398\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 52s 418ms/step - loss: 0.4293 - accuracy: 0.8724 - val_loss: 0.5261 - val_accuracy: 0.8441\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 50s 398ms/step - loss: 0.4106 - accuracy: 0.8779 - val_loss: 0.5114 - val_accuracy: 0.8489\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 52s 418ms/step - loss: 0.3932 - accuracy: 0.8821 - val_loss: 0.5029 - val_accuracy: 0.8501\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 53s 424ms/step - loss: 0.3772 - accuracy: 0.8868 - val_loss: 0.4933 - val_accuracy: 0.8540\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 50s 398ms/step - loss: 0.3623 - accuracy: 0.8911 - val_loss: 0.4848 - val_accuracy: 0.8560\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 51s 410ms/step - loss: 0.3482 - accuracy: 0.8951 - val_loss: 0.4758 - val_accuracy: 0.8586\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 50s 400ms/step - loss: 0.3346 - accuracy: 0.8992 - val_loss: 0.4739 - val_accuracy: 0.8599\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 50s 397ms/step - loss: 0.3221 - accuracy: 0.9028 - val_loss: 0.4657 - val_accuracy: 0.8623\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 51s 404ms/step - loss: 0.3101 - accuracy: 0.9061 - val_loss: 0.4620 - val_accuracy: 0.8644\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 30s 240ms/step - loss: 0.2987 - accuracy: 0.9095 - val_loss: 0.4580 - val_accuracy: 0.8654\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 27s 217ms/step - loss: 0.2878 - accuracy: 0.9129 - val_loss: 0.4549 - val_accuracy: 0.8671\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 27s 217ms/step - loss: 0.2780 - accuracy: 0.9158 - val_loss: 0.4529 - val_accuracy: 0.8694\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 29s 236ms/step - loss: 0.2683 - accuracy: 0.9186 - val_loss: 0.4551 - val_accuracy: 0.8690\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 32s 253ms/step - loss: 0.2590 - accuracy: 0.9213 - val_loss: 0.4512 - val_accuracy: 0.8705\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 41s 324ms/step - loss: 0.2505 - accuracy: 0.9238 - val_loss: 0.4528 - val_accuracy: 0.8712\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 42s 332ms/step - loss: 0.2418 - accuracy: 0.9262 - val_loss: 0.4550 - val_accuracy: 0.8713\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 41s 329ms/step - loss: 0.2340 - accuracy: 0.9285 - val_loss: 0.4576 - val_accuracy: 0.8711\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 41s 330ms/step - loss: 0.2263 - accuracy: 0.9312 - val_loss: 0.4553 - val_accuracy: 0.8724\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 41s 329ms/step - loss: 0.2191 - accuracy: 0.9333 - val_loss: 0.4606 - val_accuracy: 0.8725\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 43s 341ms/step - loss: 0.2119 - accuracy: 0.9353 - val_loss: 0.4655 - val_accuracy: 0.8715\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 41s 330ms/step - loss: 0.2055 - accuracy: 0.9372 - val_loss: 0.4660 - val_accuracy: 0.8723\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 41s 329ms/step - loss: 0.1993 - accuracy: 0.9391 - val_loss: 0.4696 - val_accuracy: 0.8724\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 44s 354ms/step - loss: 0.1928 - accuracy: 0.9412 - val_loss: 0.4668 - val_accuracy: 0.8738\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 41s 332ms/step - loss: 0.1871 - accuracy: 0.9429 - val_loss: 0.4738 - val_accuracy: 0.8728\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 43s 348ms/step - loss: 0.1816 - accuracy: 0.9447 - val_loss: 0.4750 - val_accuracy: 0.8731\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 44s 348ms/step - loss: 0.1761 - accuracy: 0.9463 - val_loss: 0.4794 - val_accuracy: 0.8730\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 42s 334ms/step - loss: 0.1709 - accuracy: 0.9479 - val_loss: 0.4834 - val_accuracy: 0.8728\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 43s 344ms/step - loss: 0.1664 - accuracy: 0.9489 - val_loss: 0.4871 - val_accuracy: 0.8736\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 42s 338ms/step - loss: 0.1616 - accuracy: 0.9504 - val_loss: 0.4894 - val_accuracy: 0.8738\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 30s 244ms/step - loss: 0.1569 - accuracy: 0.9520 - val_loss: 0.4916 - val_accuracy: 0.8738\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 33s 261ms/step - loss: 0.1527 - accuracy: 0.9529 - val_loss: 0.5011 - val_accuracy: 0.8730\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 45s 357ms/step - loss: 0.1490 - accuracy: 0.9541 - val_loss: 0.5023 - val_accuracy: 0.8732\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 32s 255ms/step - loss: 0.1445 - accuracy: 0.9554 - val_loss: 0.5087 - val_accuracy: 0.8726\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 29s 232ms/step - loss: 0.1410 - accuracy: 0.9566 - val_loss: 0.5088 - val_accuracy: 0.8740\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 29s 236ms/step - loss: 0.1374 - accuracy: 0.9575 - val_loss: 0.5162 - val_accuracy: 0.8728\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 30s 237ms/step - loss: 0.1338 - accuracy: 0.9588 - val_loss: 0.5249 - val_accuracy: 0.8725\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.1303 - accuracy: 0.9596 - val_loss: 0.5245 - val_accuracy: 0.8730\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 29s 232ms/step - loss: 0.1275 - accuracy: 0.9603 - val_loss: 0.5276 - val_accuracy: 0.8734\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 30s 237ms/step - loss: 0.1239 - accuracy: 0.9616 - val_loss: 0.5289 - val_accuracy: 0.8735\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 28s 226ms/step - loss: 0.1212 - accuracy: 0.9624 - val_loss: 0.5381 - val_accuracy: 0.8731\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 27s 213ms/step - loss: 0.1183 - accuracy: 0.9631 - val_loss: 0.5474 - val_accuracy: 0.8726\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 32s 255ms/step - loss: 0.1155 - accuracy: 0.9642 - val_loss: 0.5423 - val_accuracy: 0.8725\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 36s 290ms/step - loss: 0.1129 - accuracy: 0.9648 - val_loss: 0.5483 - val_accuracy: 0.8740\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 37s 294ms/step - loss: 0.1101 - accuracy: 0.9655 - val_loss: 0.5556 - val_accuracy: 0.8728\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.1077 - accuracy: 0.9663 - val_loss: 0.5617 - val_accuracy: 0.8729\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 28s 226ms/step - loss: 0.1056 - accuracy: 0.9666 - val_loss: 0.5651 - val_accuracy: 0.8723\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.1032 - accuracy: 0.9674 - val_loss: 0.5743 - val_accuracy: 0.8727\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 32s 255ms/step - loss: 0.1008 - accuracy: 0.9681 - val_loss: 0.5746 - val_accuracy: 0.8729\n",
      "Epoch 57/100\n",
      "125/125 [==============================] - 28s 221ms/step - loss: 0.0987 - accuracy: 0.9689 - val_loss: 0.5806 - val_accuracy: 0.8721\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 27s 215ms/step - loss: 0.0964 - accuracy: 0.9694 - val_loss: 0.5852 - val_accuracy: 0.8718\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 28s 221ms/step - loss: 0.0948 - accuracy: 0.9699 - val_loss: 0.5888 - val_accuracy: 0.8719\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 36s 286ms/step - loss: 0.0925 - accuracy: 0.9708 - val_loss: 0.5911 - val_accuracy: 0.8712\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 32s 257ms/step - loss: 0.0908 - accuracy: 0.9709 - val_loss: 0.5954 - val_accuracy: 0.8712\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 28s 227ms/step - loss: 0.0890 - accuracy: 0.9716 - val_loss: 0.6105 - val_accuracy: 0.8709\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 36s 287ms/step - loss: 0.0873 - accuracy: 0.9721 - val_loss: 0.6040 - val_accuracy: 0.8723\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 28s 227ms/step - loss: 0.0850 - accuracy: 0.9728 - val_loss: 0.6090 - val_accuracy: 0.8715\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 28s 221ms/step - loss: 0.0837 - accuracy: 0.9731 - val_loss: 0.6110 - val_accuracy: 0.8721\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 28s 227ms/step - loss: 0.0823 - accuracy: 0.9735 - val_loss: 0.6138 - val_accuracy: 0.8723\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 31s 245ms/step - loss: 0.0804 - accuracy: 0.9741 - val_loss: 0.6187 - val_accuracy: 0.8720\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 44s 355ms/step - loss: 0.0790 - accuracy: 0.9745 - val_loss: 0.6244 - val_accuracy: 0.8721\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 48s 383ms/step - loss: 0.0772 - accuracy: 0.9750 - val_loss: 0.6296 - val_accuracy: 0.8702\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 29s 234ms/step - loss: 0.0764 - accuracy: 0.9754 - val_loss: 0.6317 - val_accuracy: 0.8714\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 28s 224ms/step - loss: 0.0748 - accuracy: 0.9758 - val_loss: 0.6369 - val_accuracy: 0.8720\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 28s 220ms/step - loss: 0.0731 - accuracy: 0.9762 - val_loss: 0.6410 - val_accuracy: 0.8712\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 28s 220ms/step - loss: 0.0718 - accuracy: 0.9764 - val_loss: 0.6483 - val_accuracy: 0.8713\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 35s 277ms/step - loss: 0.0707 - accuracy: 0.9767 - val_loss: 0.6529 - val_accuracy: 0.8709\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 35s 276ms/step - loss: 0.0695 - accuracy: 0.9772 - val_loss: 0.6535 - val_accuracy: 0.8713\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 31s 250ms/step - loss: 0.0680 - accuracy: 0.9776 - val_loss: 0.6536 - val_accuracy: 0.8710\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 44s 356ms/step - loss: 0.0667 - accuracy: 0.9778 - val_loss: 0.6720 - val_accuracy: 0.8706\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 33s 265ms/step - loss: 0.0658 - accuracy: 0.9783 - val_loss: 0.6644 - val_accuracy: 0.8715\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 28s 221ms/step - loss: 0.0645 - accuracy: 0.9786 - val_loss: 0.6679 - val_accuracy: 0.8701\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 28s 226ms/step - loss: 0.0635 - accuracy: 0.9788 - val_loss: 0.6738 - val_accuracy: 0.8708\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 49s 389ms/step - loss: 0.0625 - accuracy: 0.9791 - val_loss: 0.6805 - val_accuracy: 0.8697\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.0611 - accuracy: 0.9795 - val_loss: 0.6826 - val_accuracy: 0.8707\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 28s 225ms/step - loss: 0.0601 - accuracy: 0.9799 - val_loss: 0.6875 - val_accuracy: 0.8699\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 31s 246ms/step - loss: 0.0591 - accuracy: 0.9800 - val_loss: 0.6899 - val_accuracy: 0.8702\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 29s 231ms/step - loss: 0.0580 - accuracy: 0.9803 - val_loss: 0.6907 - val_accuracy: 0.8701\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 29s 235ms/step - loss: 0.0573 - accuracy: 0.9804 - val_loss: 0.6981 - val_accuracy: 0.8699\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 29s 233ms/step - loss: 0.0565 - accuracy: 0.9808 - val_loss: 0.7006 - val_accuracy: 0.8700\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 28s 225ms/step - loss: 0.0554 - accuracy: 0.9812 - val_loss: 0.7022 - val_accuracy: 0.8706\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 28s 223ms/step - loss: 0.0547 - accuracy: 0.9813 - val_loss: 0.7116 - val_accuracy: 0.8695\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 28s 223ms/step - loss: 0.0538 - accuracy: 0.9814 - val_loss: 0.7122 - val_accuracy: 0.8692\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 28s 223ms/step - loss: 0.0531 - accuracy: 0.9819 - val_loss: 0.7052 - val_accuracy: 0.8708\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 28s 223ms/step - loss: 0.0524 - accuracy: 0.9818 - val_loss: 0.7126 - val_accuracy: 0.8705\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 28s 223ms/step - loss: 0.0517 - accuracy: 0.9823 - val_loss: 0.7132 - val_accuracy: 0.8702\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 28s 222ms/step - loss: 0.0505 - accuracy: 0.9824 - val_loss: 0.7200 - val_accuracy: 0.8699\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 28s 222ms/step - loss: 0.0501 - accuracy: 0.9826 - val_loss: 0.7236 - val_accuracy: 0.8697\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 28s 223ms/step - loss: 0.0494 - accuracy: 0.9828 - val_loss: 0.7231 - val_accuracy: 0.8701\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 28s 222ms/step - loss: 0.0484 - accuracy: 0.9829 - val_loss: 0.7307 - val_accuracy: 0.8700\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 28s 222ms/step - loss: 0.0481 - accuracy: 0.9833 - val_loss: 0.7375 - val_accuracy: 0.8694\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 28s 222ms/step - loss: 0.0474 - accuracy: 0.9834 - val_loss: 0.7377 - val_accuracy: 0.8681\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 28s 222ms/step - loss: 0.0470 - accuracy: 0.9835 - val_loss: 0.7362 - val_accuracy: 0.8688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdb006f16d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# 'encoder_input_data' & 'decoder_input_data' into 'decoder_target_data'\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For inference,\n",
    "1) Encode the input sentence and retrieve the initial decoder state \n",
    "\n",
    "2) Run one step of the decoder with this initial state and a \"start of sequence\" token as target. The output will be the next target character.\n",
    "\n",
    "3) Append the target character predicted and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is our inference setup\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape = (latent_dim, ))\n",
    "decoder_state_input_c = Input(shape = (latent_dim, ))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reverse_target_char_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7602936fdc3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-ef90cea883ca>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Sample a token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0msampled_token_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msampled_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreverse_target_char_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msampled_token_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msampled_char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reverse_target_char_index' is not defined"
     ]
    }
   ],
   "source": [
    "decode_sequence(encoder_input_data[0].reshape([1,encoder_input_data[0].shape[0], encoder_input_data[0].shape[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 71)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
